{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of convolutions with tensorflow\n",
    "\n",
    "In this notebook, you'll be using tensorflow to build a Convolutional Neural Network (CNN).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "Both, [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/Convolution.ipynb) and this [wikipedia page](https://en.wikipedia.org/wiki/Convolution) might help you understand what is a convolution.\n",
    "\n",
    "no, if we consider two functions $f$ and $g$ taking values from $\\mathbb{Z} \\to \\mathbb{R}$ then:  \n",
    "$ (f * g)[n] = \\sum_{m = -\\infty}^{+\\infty} f[m] \\cdot g[n - m] $\n",
    "\n",
    "In our case, we consider the two vectors $x$ and $w$ :  \n",
    "$ x = (x_1, x_2, ..., x_{n-1}, x_n) $  \n",
    "$ w = (w_1, w_2) $\n",
    "\n",
    "And get :   \n",
    "$ x * w = (w_1 x_1 + w_2 x_2, w_1 x_2 + w_2 x_3, ..., w_1 x_{n-1} + w_2 x_n)$\n",
    "\n",
    "\n",
    "#### Deep learning subtility :\n",
    "    \n",
    "In most of deep learning framewoks, you'll get to chose in between three paddings:\n",
    "- **Same**: $(f*g)$ has the same shape as x (we pad the entry with zeros)\n",
    "- **valid**: $(f*g)$ has the shape of x minus the shape of w plus 1 (no padding on x)\n",
    "- **Causal**: $(f*g)(n_t)$ does not depend on any $(n_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow\n",
    "\n",
    "\"TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and also used for machine learning applications such as neural networks.[3] It is used for both research and production at Google often replacing its closed-source predecessor, DistBelief.\" - Wikipedia\n",
    "\n",
    "We'll be using tensorflow to build the models we want to use. \n",
    "\n",
    "Here below, we build a AND gate with a very simple neural network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00839782]\n",
      " [ 0.14990877]\n",
      " [ 0.14990877]\n",
      " [ 0.78595549]] Tensor(\"Y:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our Dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([0,0,0,1]).reshape(-1,1)\n",
    "\n",
    "\n",
    "# Define the tensorflow tensors\n",
    "x = tf.placeholder(tf.float32, [None, 2], name='X')  # inputs\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='Y')  # outputs\n",
    "W = tf.Variable(tf.zeros([2, 1]), name='W')\n",
    "b = tf.Variable(tf.zeros([1,]), name='b')\n",
    "\n",
    "# Define the model\n",
    "pred = tf.nn.sigmoid(tf.matmul(x, W) + b)  # Model\n",
    "\n",
    "# Define the loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred) + (1-y) * tf.log(1-pred), reduction_indices=1))\n",
    "    \n",
    "\n",
    "# Define the optimizer method you want to use\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# Include some Tensorboard visualization\n",
    "writer_train = tf.summary.FileWriter(\"./my_model/\")\n",
    "loss_summary = tf.summary.scalar('loss',loss)\n",
    "\n",
    "\n",
    "# Start training session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer_train.add_graph(sess.graph)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        _, c, p, ls = sess.run([optimizer, loss, pred, loss_summary], feed_dict={x: X,\n",
    "                                                      y: Y})\n",
    "        writer_train.add_summary(ls,epoch)\n",
    "print(p, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the graph you just created, launch tensorbord.  \n",
    "`$tensorboard --logdirs=./` on linux (with corresponding logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Get inspiration from the preceding code to build a XOR gate\n",
    "\n",
    "Design a neural network with 2 layers.\n",
    "- layer1 has 2 neurons (sigmoid or tanh activation)\n",
    "- Layer2 has 1 neuron (it outouts the prediction)\n",
    "\n",
    "And train  it\n",
    "\n",
    "It's **mandatory** that you get a **tensorboard visualization** of your graph, try to make it look good, plz :)\n",
    "\n",
    "Here below I put a graph of the model you want to have (yet your weights won't be the same)\n",
    "![graph](https://i.stack.imgur.com/nRZ6z.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02839497]\n",
      " [ 0.98323745]\n",
      " [ 0.98305202]\n",
      " [ 0.03247236]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "### Code here\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "logdir = \"./xor/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our Dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([0,1,1,0]).reshape(-1,1)\n",
    "\n",
    "\n",
    "# Define the tensorflow tensors\n",
    "x = tf.placeholder(tf.float32, [None, 2], name='X')  # inputs\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='Y')  # outputs\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([2,]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([1,]), name='b2')\n",
    "\n",
    "# Define the model\n",
    "A1 = tf.nn.tanh(tf.matmul(x, W1) + b1)  # Model\n",
    "pred = tf.nn.sigmoid(tf.matmul(A1, W2) + b2)\n",
    "\n",
    "# Define the loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred) + (1-y) * tf.log(1-pred), reduction_indices=1))\n",
    "    \n",
    "\n",
    "# Define the optimizer method you want to use\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# Include some Tensorboard visualization\n",
    "writer_train = tf.summary.FileWriter(logdir)\n",
    "loss_summary = tf.summary.scalar('loss',loss)\n",
    "variables_names =[v.name for v in tf.trainable_variables()]\n",
    "\n",
    "\n",
    "# Start training session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer_train.add_graph(sess.graph)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        _, c, p, ls = sess.run([optimizer, loss, pred, loss_summary], feed_dict={x: X,\n",
    "                                                      y: Y})\n",
    "        writer_train.add_summary(ls,epoch)\n",
    "        values = sess.run(variables_names)\n",
    "        \n",
    "print(p, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the weights of your model\n",
    "And give an interpretation on what they are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0 [[ 3.75516963  4.00827408]\n",
      " [-3.90196252 -3.92286706]]\n",
      "b1:0 [-1.79565525  1.89936256]\n",
      "W2:0 [[ 6.94423485]\n",
      " [-6.92984962]]\n",
      "b2:0 [ 6.42972422]\n"
     ]
    }
   ],
   "source": [
    "for k,v in zip(variables_names, values):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Build a CNN to predict the MNIST digits\n",
    "You can now move to CNNs. You'll have to train a convolutional neural network to predict the digits from MNIST.\n",
    "\n",
    "You might want to reuse some pieces of code from [SNN](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/Intro_to_SNN.ipynb)\n",
    "\n",
    "Your model should have 3 layers:\n",
    "- 1st layer : 6 convolutional kernels with shape (3,3)\n",
    "- 2nd layer : 6 convolutional kernels with shape (3,3)\n",
    "- 3rd layer : Softmax layer\n",
    "\n",
    "Train your model.\n",
    "\n",
    "Explain all you do, and why, make it lovely to read, plz o:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import pickle, gzip, numpy\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (50000, 28, 28, 1) (50000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "patch_size = 3\n",
    "depth = 6\n",
    "image_size = 28\n",
    "num_steps = 1200\n",
    "\n",
    "\n",
    "#import the dataset\n",
    "f = gzip.open('../mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
    "f.close()\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_valid, y_valid = valid_set[0], valid_set[1]\n",
    "X_test,  y_test  = test_set[0],  test_set[1]\n",
    "\n",
    "##transform labels into arrays for training\n",
    "def to_one_hot(y, n_classes=10):\n",
    "    _y = np.zeros((len(y), n_classes))\n",
    "    _y[np.arange(len(y)), y] = 1\n",
    "    return _y\n",
    "\n",
    "#rehape 2d dataset into 4d and label \n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, 1)).astype(np.float32)\n",
    "  labels = to_one_hot(labels, n_classes=10)\n",
    "  return dataset, labels\n",
    "\n",
    "X_train, y_train = reformat(X_train, y_train)\n",
    "X_valid, y_valid = reformat(X_valid, y_valid)\n",
    "X_test, y_test = reformat(X_test, y_test)\n",
    "\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_valid.shape, y_valid.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 7.032909\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 200: 0.155732\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 400: 0.359502\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 600: 0.041322\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 96.0%\n",
      "Minibatch loss at step 800: 0.187320\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 95.8%\n",
      "Minibatch loss at step 1000: 0.161814\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 95.7%\n",
      "Test accuracy: 96.1%\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "logdir = \"./CNN/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#calculate accuracy\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, 1))\n",
    "train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "valid_dataset = tf.constant(X_valid)\n",
    "test_dataset = tf.constant(X_test)\n",
    "\n",
    "# define placeholder for weights and biases to network\n",
    "W1 = tf.Variable(tf.random_normal(\n",
    "      [3, 3, 1, 6], stddev=0.1),name = 'W1') # patch 3x3, in size 1, out size 6\n",
    "b1 = tf.Variable(tf.random_normal([6]),name='b1')\n",
    "W2 = tf.Variable(tf.random_normal(\n",
    "      [3, 3, 6, 36], stddev=0.1),name='W2') # patch 3x3, in size 6, out size 6\n",
    "b2 = tf.Variable(tf.random_normal([36]),name='b2')\n",
    "W3 = tf.Variable(tf.random_normal(\n",
    "      [7 * 7 * 36,10], stddev=0.1),name= 'W3') # patch 3x3, in size 6, out size 6\n",
    "b3 = tf.Variable(tf.random_normal([10]),name='b3')\n",
    "\n",
    "#downsample the image data extracted by the convolutional layers\n",
    "#to reduce the dimensionality of the feature map in order to decrease processing time.\n",
    "def maxpool2d(x):\n",
    "    return tf.layers.max_pooling2d(x,pool_size=[2,2], strides=2)\n",
    "\n",
    "# Model.\n",
    "def model(data):\n",
    "  with tf.name_scope('layers'):\n",
    "    # compute layer 1 \n",
    "    with tf.name_scope('A1'):\n",
    "        conv1 = tf.nn.conv2d(data, W1, [1, 1, 1, 1], padding='SAME')\n",
    "        A1 = tf.nn.elu(conv1 + b1)\n",
    "        A1 = maxpool2d(A1)\n",
    "        variable_summaries(A1)\n",
    "    #compute layer 2\n",
    "    with tf.name_scope('A2'):\n",
    "        conv2 = tf.nn.conv2d(A1, W2, [1, 1, 1, 1], padding='SAME')\n",
    "        A2 = tf.nn.elu(conv2 + b2)\n",
    "        A2 = maxpool2d(A2)\n",
    "        variable_summaries(A2)\n",
    "    #reshape matrix for ouput\n",
    "    reshape = tf.reshape(A2,[-1, 7 * 7 * 36] )\n",
    "    #Prevent overfitting with drouput regularization\n",
    "    dropout= tf.nn.dropout(reshape, keep_prob=0.5)\n",
    "    return (tf.matmul(dropout, W3) + b3)\n",
    "\n",
    "#summaries for tensorboard\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    #tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "\n",
    "\n",
    "output = model(train_dataset)\n",
    "# the error between prediction and real data\n",
    "with tf.name_scope(\"loss\"):\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=train_labels, logits=output))\n",
    "  \n",
    "# Define the optimizer method you want to use\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "# Include some Tensorboard visualization\n",
    "writer_train = tf.summary.FileWriter(logdir)\n",
    "loss_summary = tf.summary.scalar('loss_summary',loss)\n",
    "\n",
    "        \n",
    "  # Predictions for the training, validation, and test data.\n",
    "with tf.name_scope('predictions'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_prediction = tf.nn.softmax(output)\n",
    "        variable_summaries(train_prediction)\n",
    "    with tf.name_scope('valid'):\n",
    "        valid_prediction = tf.nn.softmax(model(valid_dataset))\n",
    "        variable_summaries(valid_prediction)\n",
    "    with tf.name_scope('test'):\n",
    "        test_prediction = tf.nn.softmax(model(test_dataset))\n",
    "        variable_summaries(test_prediction)\n",
    "\n",
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "variables_names =[v.name for v in tf.trainable_variables()]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer_train.add_graph(sess.graph)\n",
    "        \n",
    "    for step in range(num_steps):        \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = X_train[offset:(offset + batch_size), :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # key of the dictionary is placeholder node of the graph to be fed,\n",
    "        # and value is the numpy array to feed to it.\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l, predictions, ls= sess.run(\n",
    "          [optimizer, loss, train_prediction,loss_summary], feed_dict=feed_dict)\n",
    "        \n",
    "        #writer_train.add_summary(summary,step)\n",
    "        if (step % 200 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(\n",
    "            predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), y_valid))\n",
    "        \n",
    "        #acc = accuracy(predictions, batch_labels)\n",
    "        writer_train.add_summary(ls,step)\n",
    "    \n",
    "    values = sess.run(variables_names)\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the weights of your model\n",
    "And give an interpretation on what they are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0 (3, 3, 1, 6)\n",
      "b1:0 (6,)\n",
      "W2:0 (3, 3, 6, 36)\n",
      "b2:0 (36,)\n",
      "W3:0 (1764, 10)\n",
      "b3:0 (10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Each weight holds a generalized matrix \\nthat represents the importance of each pixel in the image\\nThose weights are updated to minimize the loss at each iteration'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in zip(variables_names, values):\n",
    "    print(k, v.shape)\n",
    "\"\"\"Each weight holds a generalized matrix \n",
    "that represents the importance of each pixel in the image\n",
    "Those weights are updated to minimize the loss at each iteration\n",
    "The depth as the complexity of convolutions increase to keep only parameters that map to content\n",
    "of the image remain\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose one (tell me what you chose...)\n",
    "- Show how the gradients (show only one kernel) evolve for good and wrong prediction. (hard)\n",
    "- Initialize the kernels with values that make sense for you and show how they evolve. (easy) \n",
    "- When training is finished, show the 6+6=12 results of some convolved immages. (easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADYAAAD8CAYAAAA8NlcPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8d5bc65c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(values[4], interpolation='nearest')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
